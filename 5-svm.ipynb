{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitbasecondafcf336e26a72429c81f60a8c41a13425",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this notebook, we'll be implementing a SVM to use to classify the CIFAR dataset.\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "#Let's load our data - this is identical to what we did in the knn notebook.\n",
    "with open(\"testTrainLab1.pickle\", \"rb\") as f:\n",
    "    labData = pickle.load(f)\n",
    "X_train = np.reshape(labData[\"X_train\"], (labData[\"X_train\"].shape[0], -1))\n",
    "X_test = np.reshape(labData[\"X_test\"], (labData[\"X_test\"].shape[0], -1))\n",
    "y_train = labData[\"y_train\"]\n",
    "y_test = labData[\"y_test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 3072)\n",
      "(40000, 3072)\n",
      "(1000, 3072)\n"
     ]
    }
   ],
   "source": [
    "#Let's cut our data up into training, validation, and testing subsets.  W\n",
    "#As a good practice, we're also going to create a small dataset to use for\n",
    "#code development - otherwise we have to wait for a long time before errors\n",
    "#become evident!\n",
    "\n",
    "#Remember, CIFAR10 has 50,000 training cases, and 10,000 testing cases.\n",
    "trainingSize = 40000\n",
    "validationSize = 5000\n",
    "testingSize = 5000\n",
    "\n",
    "#You can make the dev size smaller or bigger, depending on how powerful your computer is:\n",
    "devSize = 1000\n",
    "\n",
    "#CIFAR batches are random, so we can just take sequential cases.  Here is our validation set,\n",
    "#taking the last 10,000 observations (indices 40000 - 50000 are in the mask)\n",
    "valMask = range(trainingSize, 50000)\n",
    "X_val = X_train[valMask]\n",
    "y_val = y_train[valMask]\n",
    "\n",
    "#This will now be a 10,000 by 3,072 shape vector - representing 10,000 images with \n",
    "#3,072 pixels each (32*32*3, for three bands of color)\n",
    "print(X_val.shape)\n",
    "\n",
    "#Training - here we take the first 40k (removing the validation data from our training data).\n",
    "#Note re-using X_train and y_train here is not good practice - i.e., this cell will not run\n",
    "#the second time because X_train and y_train have been redefined.\n",
    "#However, using these variables will make our code broadly consistent with most examples\n",
    "#you will find, so we are leaving these as-is.  In your own implementations, you may want to\n",
    "#consider naming this variables something else (i.e., \"X_train_40ksubset\").\n",
    "trainMask = range(0, trainingSize)\n",
    "X_train = X_train[trainMask]\n",
    "y_train = y_train[trainMask]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "#Development set - just a tiny copy of some of the development data to use as a test.\n",
    "devMask = range(0, devSize)\n",
    "X_dev = X_train[devMask]\n",
    "y_dev = y_train[devMask]\n",
    "\n",
    "print(X_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'dataLoss': 10.42042989605346, 'percentCorrect': 0.127}\n"
     ]
    }
   ],
   "source": [
    "#Now we're going to write a simple SVM classifier\n",
    "#In this first case, we won't have any optimization -\n",
    "#We will write a function that takes \n",
    "#X (images), y (labels), W (weights), and an epsilon value (e) as inputs,\n",
    "#and then informs us what the data loss would be:\n",
    "\n",
    "def simpleSVM(X, y, W, e):\n",
    "    #Lists to hold our losses\n",
    "    dataLoss = []\n",
    "\n",
    "    #Counter to report accuracy\n",
    "    correct = 0\n",
    "\n",
    "    #Create some basic counters- note this is unnecessary,\n",
    "    #but helpful to improve the readability to the code.\n",
    "    countClasses = W.shape[1]\n",
    "    countTrainSamples = X.shape[0]\n",
    "\n",
    "    #Iterate over every input X to predict classes\n",
    "    for i in range(countTrainSamples):\n",
    "        scores = X[i].dot(W)\n",
    "        trueClassScore = scores[y[i]]\n",
    "\n",
    "        #Determine if it was correct or not:\n",
    "        if(trueClassScore == np.max(scores)):\n",
    "            correct = correct + 1\n",
    "\n",
    "        #Calculate the loss for each case\n",
    "        loss_i = 0\n",
    "        for j in range(countClasses):\n",
    "            if(j != y[i]):\n",
    "                loss_i = max(scores[j] - trueClassScore + e, 0) + loss_i\n",
    "        \n",
    "        dataLoss.append(loss_i)\n",
    "    \n",
    "    #Return the total average loss across all samples, along with the percent correct for interpretation\n",
    "    return({\"dataLoss\":np.sum(dataLoss) / countTrainSamples, \"percentCorrect\":correct/countTrainSamples})\n",
    "\n",
    "\n",
    "#Here, we create an entirely random set of weights to test.\n",
    "#We specifically are creating a 3072 * 10 matrix - one weight for every pixel, for every class.\n",
    "#Right now, this will (obviously) result in a pretty bad classification.\n",
    "#Once we start working with optimization, we'll work to improve this - but random is fine for now.\n",
    "W = np.random.randn(3072, 10) * 0.0001 \n",
    "\n",
    "dataLoss = simpleSVM(X_dev, y_dev, W, 1.0)\n",
    "print(dataLoss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'dataLoss': 10.874920734785697, 'regLoss': 0.00030475322446098924, 'totalLoss': 10.875225488010157, 'percentCorrect': 0.089}\n"
     ]
    }
   ],
   "source": [
    "#Now, let's do the same thing, but add a regularization term in.  We'll use a L2 norm in this example.\n",
    "\n",
    "#First, we add the lambda term to determine the tradeoff between\n",
    "#our data annd regularization loss.\n",
    "def simpleSVMWithReg(X, y, W, e, l):\n",
    "    dataLoss = []\n",
    "    correct = 0\n",
    "    countClasses = W.shape[1]\n",
    "    countTrainSamples = X.shape[0]\n",
    "\n",
    "    for i in range(countTrainSamples):\n",
    "        scores = X[i].dot(W)\n",
    "        trueClassScore = scores[y[i]]\n",
    "\n",
    "        if(trueClassScore == np.max(scores)):\n",
    "            correct = correct + 1\n",
    "\n",
    "        loss_i = 0\n",
    "        for j in range(countClasses):\n",
    "            if(j != y[i]):\n",
    "                loss_i = max(0, (scores[j] - trueClassScore + e)) + loss_i\n",
    "        dataLoss.append(loss_i)\n",
    "\n",
    "    #Here is our data loss function, as before:\n",
    "    dataLoss = np.sum(dataLoss) / countTrainSamples\n",
    "\n",
    "    #Regularization Loss\n",
    "    regLoss = np.sum(W*W)\n",
    "\n",
    "    #Total Loss\n",
    "    totalLoss = dataLoss + (l * regLoss)\n",
    "    return({\"dataLoss\":dataLoss, \"regLoss\":regLoss, \"totalLoss\":totalLoss, \"percentCorrect\":correct/countTrainSamples})\n",
    "\n",
    "W = np.random.randn(3072, 10) * 0.0001 \n",
    "\n",
    "dataLoss = simpleSVMWithReg(X_dev, y_dev, W, e=1.0, l=1.0)\n",
    "print(dataLoss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====Vectorized Implementation====\nTrue class score for observation 20: -1.600041308191975\nCIFAR Class 1 Score for Observation 20: -0.7394481065723721\nLoss for first contrast of 20th observation: 1.860593201619603\nTotal loss for 20th observation:21.780492802827126\n\n\n====For Loop Implementation====\nTrue class score for observation 20: -1.6000413081919749\nCIFAR Class 1 Score for Observation 20: -0.7394481065723724\nLoss for first contrast of 20th observation: 1.8605932016196025\nTotal loss for 20th observation: 21.780492802827123\n\n\n\n{'dataLoss_vectorized': 10.429355366253667, 'dataLoss_forLoop': 10.429355366253667, 'regLoss': 0.0003060647029503595, 'totalLoss': 10.429661430956617, 'percentCorrect': 0.102}\n"
     ]
    }
   ],
   "source": [
    "#The above does what we need, but it's very ineffecient due to our use of\n",
    "#for loops.  The below implementation is functionally identical,\n",
    "#but uses a vectorized implementation.\n",
    "\n",
    "def simpleSVMEffecient(X, y, W, e, l):\n",
    "    dataLoss = []\n",
    "    correct = 0\n",
    "\n",
    "    #Here, we compute scores for the entire set of observations at once\n",
    "    #Instead of looping over them\n",
    "    scores = X.dot(W) #<--- 1000 x 10 column, representing the 1000 cases in our dev dataset and 10 classes.\n",
    "    countTrainSamples = scores.shape[0] #1000\n",
    "    countClasses = scores.shape[1] #10\n",
    "    \n",
    "    #Now we need to look up the trueClassScore for everything - i.e., if y = Cat, we need the score for cat.\n",
    "    #Remember - y is a number from 0 to 9, representing the 10 classes.\n",
    "    #So, to find the score for y in the matrix for each of 1000 rows,\n",
    "    #we look at the column y.  That's what the below function does.\n",
    "    #np.arange just creates a list from 0 to 1000, and y is in the same order,\n",
    "    #giving us the lookups.\n",
    "    \n",
    "    trueClassScores = scores[np.arange(scores.shape[0]), y]\n",
    "    print(\"====Vectorized Implementation====\")\n",
    "    print(\"True class score for observation 20: \" + str(trueClassScores[20]))\n",
    "    \n",
    "    #Calculate the difference between each incorrect class score and the true class score\n",
    "    trueClassMatrix = np.matrix(trueClassScores).T #Converting our array to a 1000 by 1 matrix, to allow for matrix manipulation.\n",
    "    #print(trueClassMatrix[20,0]) #<--- Will be the same as trueClassScores[20] above.\n",
    "\n",
    "    #Subtract the true class value from every element in the scores matrix:\n",
    "    loss_ij = np.maximum(0, (scores - trueClassMatrix) + e) #1000 x 10 matrix\n",
    "    \n",
    "    print(\"CIFAR Class 1 Score for Observation 20: \" + str(scores[20,0]))\n",
    "    \n",
    "    #This should be equivalent to:\n",
    "    #print(str(scores[20,0] - trueClassScores[20] + e))\n",
    "    print(\"Loss for first contrast of 20th observation: \" + str(loss_ij[20,0]))\n",
    "    \n",
    "    #Remove the cases where we compare the true class to the true class (we only want the other cases for the SVM loss)\n",
    "    loss_ij[np.arange(countTrainSamples), y] = 0\n",
    "\n",
    "    #Note - this entire block of code is unnecessary, but helpful to illustrate what's going on.\n",
    "    exampleLossObs20 = np.sum(loss_ij[20])\n",
    "    print(\"Total loss for 20th observation:\" + str(exampleLossObs20))\n",
    "\n",
    "    #Calculate the mean data loss\n",
    "    dataLossNew = np.sum(np.sum(loss_ij)) / countTrainSamples\n",
    "\n",
    "    #The below function is identical to what we calculated before:\n",
    "    print(\"\\n\\n====For Loop Implementation====\")\n",
    "    for i in range(countTrainSamples):\n",
    "        scores = X[i].dot(W)\n",
    "        trueClassScore = scores[y[i]]\n",
    "\n",
    "        #===============\n",
    "        #For comparison to vectorized implementation:\n",
    "        if(i == 20):\n",
    "            print(\"True class score for observation 20: \" + str(trueClassScore))\n",
    "        #===============\n",
    "\n",
    "        if(trueClassScore == np.max(scores)):\n",
    "            correct = correct + 1\n",
    "\n",
    "        loss_i = 0\n",
    "        for j in range(countClasses):\n",
    "            \n",
    "            \n",
    "            #===============\n",
    "            #For comparison to vectorized implementation:\n",
    "            if(i == 20):\n",
    "                if(j == 0):\n",
    "                    print(\"CIFAR Class 1 Score for Observation 20: \" + str(scores[j]))\n",
    "                    print(\"Loss for first contrast of 20th observation: \" + str(max(0, scores[j] - trueClassScore + e)))\n",
    "            #===============\n",
    "\n",
    "            if(j != y[i]):\n",
    "                loss_i = max(0, (scores[j] - trueClassScore + e)) + loss_i\n",
    "        \n",
    "        #===============\n",
    "        #For comparison to vectorized implementation:\n",
    "        if(i == 20):\n",
    "            print(\"Total loss for 20th observation: \" + str(loss_i))\n",
    "        #===============\n",
    "\n",
    "        dataLoss.append(loss_i)\n",
    "\n",
    "    #Here is our data loss function, as before:\n",
    "    dataLoss_old = np.sum(dataLoss) / countTrainSamples\n",
    "\n",
    "    #Regularization Loss\n",
    "    regLoss = np.sum(W*W)\n",
    "\n",
    "    #Total Loss\n",
    "    totalLoss = dataLoss_old + (l * regLoss)\n",
    "    return({\"dataLoss_vectorized\":dataLossNew, \"dataLoss_forLoop\":dataLoss_old, \"regLoss\":regLoss, \"totalLoss\":totalLoss, \"percentCorrect\":correct/countTrainSamples})\n",
    "\n",
    "W = np.random.randn(3072, 10) * 0.0001 \n",
    "\n",
    "dataLoss = simpleSVMEffecient(X_dev, y_dev, W, e=1.0, l=1.0)\n",
    "print(\"\\n\\n\")\n",
    "print(dataLoss)"
   ]
  }
 ]
}